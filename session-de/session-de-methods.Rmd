---
title: "Differential expression methods"
bibliography: bibliography.bib
#output: github_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path="session-de-files/figures/")
```

## Learning objectives
- describe main classes of methods used
- understand statistical concepts behind the key scRNA-seq DE methods
- run the key scRNA-seq DE methods

-------

## Examples
<figure>
<img src="session-de-files/images/methods-Wang-2019.png" width="800" height="800">
<figcaption>Figure: Software tools for identifying DE genes using scRNAseq data [@Wang2019]</figcaption></figure>

#### More more examples
And even more examples in the  [Soneson, Charlotte, and Mark D Robinson](session-de-files/images/methods-Robinson-2018.pdf) [-@Soneson2018]

----------

## Main classes
#### non-parameteric tests
- generic methods
- e.g. Wilcoxon rank-sum test, Kruskal-Wallis, Kolmogorov-Smirnov test
- non-parametric tests generally convert observed expression values to ranks & test whether the distribution of ranks for one group are signficantly different from the distribution of ranks for the other group
- some non-parametric methods fail in the presence of a large number of tied values, such as the case for dropouts (zeros) in single-cell RNA-seq expression data
- if the conditions for a parametric test hold, then it will typically be more powerful than a non-parametric test

#### bulk RNA-seq based method
- developed for bulk RNA-seq
- e.g. edgeR, DE-seq2
- compare estimates of mean-expression (sample size), based on negative binomial distribution
- can be assessed by datasets where RNA-seq data has beeen validated by RT-qPCR

#### scRNA-seq specific methods
- developed for scRNA-seq
- e.g. MAST, SCDE, Monocle, Pagoda, D$^3$E etc. 
- large number of samples, i.e. cells, for each group we are comparing in single-cell experiments. Thus we can take advantage of the whole distribution of expression values in each group to identify differences between groups 
-we usually do not have a defined set of experimental conditions; instead we try to  identify the cell groups by using an unsupervised clustering approach.

#### modeling wise
- distribution free (non-parameteric)
- negative binomial
- zero inflated negative binomial
- Poisson and negative binomial
- GLM and GAM
- etc. 

--------

## Statistical thinking

<figure>
<img src="session-de-files/images/methods-stats.png" width="400" height="400">
<figcaption>$Outcome_i=(Model_i)+error_i$</figcaption>
</figure>

#### Normal distribution example
```{r, echo=F, eval=T}
set.seed(1)
x <- rnorm(1000, 170, 1)
h <-hist(x, col=rgb(1,0,0,0.5), xlab="height [cm]", n=50, main="", xlim=c(165, 180))
xfit <-seq(min(x),max(x),length=40) 
yfit <-dnorm(xfit,mean=mean(x),sd=sd(x)) 
yfit <- yfit*diff(h$mids[1:2])*length(x) 
lines(xfit, yfit, col="black", lwd=2)

x2 <- rnorm(1000, 173, 1)
h<-hist(x2, col=rgb(0,0,1,0.5), xlab="height", n=50, add=T, main="")
xfit2<-seq(min(x2),max(x2),length=40) 
yfit2<-dnorm(xfit2,mean=mean(x2),sd=sd(x2)) 
yfit2 <- yfit2*diff(h$mids[1:2])*length(x2) 
lines(xfit2, yfit2, col="black", lwd=2)
```
$t=\frac{x_1-x_2}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$

#### Generic recipe
- model e.g. gene expression with random error
- fit model to the data and/or data to the model, estimate model parameters
- use model for prediction and/or inference
- the better model fits the data the better statistics
\end{block}
\end{frame}

## Common distributions

#### Negative binomial (NB)
```{r, dist-NB, echo=F, fig.height=3}

set.seed(1)
par(mfrow=c(1,3))
hist(rnbinom(1000, mu=4, size=1), col="grey", xlab="Read Counts", main="mu=4, size=1")
hist(rnbinom(1000, mu = 4, size = 10), col="grey", xlab="Read Counts", main="mu=4, size=10")
hist(rnbinom(1000, mu = 4, size = 100), col="grey", xlab="Read Counts", main="mu=4, size=100")

```
$$\mu=mu$$
$$\delta^2=mu+mu^2/size$$
$\mu$, mean expression

_size_: and the dispersion, which is inversely related to the variance

NB fits bulk RNA-seq data very well and it is used for most statistical methods designed for such data. In addition, it has been show to fit the distribution of molecule counts obtained from data tagged by unique molecular identifiers (UMIs) quite well (Grun et al. 2014, Islam et al. 2011).

#### zero inflated NB
```{r, dist-zero-inflated-NB, echo=F, fig.height=3}

set.seed(1)
par(mfrow=c(1,2))

d = 0.5;
counts <- rnbinom(1000, mu=10, size=100);
counts[runif(1000) < d] = 0;
hist(counts, col="grey50", xlab="Read Counts", main="Zero-inflated NB", n=50);

d = 0.2;
counts <- rnbinom(1000, mu=20, size=100);
counts[runif(1000) < d] = 0;
hist(counts, col="grey50", xlab="Read Counts", main="Zero-inflated NB", n=50);

```
$$\mu=mu*(1-d)$$
$$\delta^2=\mu*(1-d)*(1+d*\mu+\mu/size)$$

_d_, dropout rate; dropout rate of a gene is strongly correlated with the mean expression of the gene. Different zero-inflated negative binomial models use different relationships between _mu_ and _d_ and some may fit _mu_ and _d_ to the expression of each gene independently. Implemented in MAST, SCDE.

#### Poisson distribution
```{r, dist-Poisson, echo=F, fig.height=3}

set.seed(1)
par(mfrow=c(1,2))

a <- 0.1
b <- 0.1
g <- 100
lambdas <- rbeta(1000, a, b)
counts <- sapply(g*lambdas, function(l) {rpois(1, lambda = l)})
hist(
    counts,
    col = "grey50",
    xlab = "Read Counts",
    main = "Poisson-Beta"
)


a <- 0.4
b <- 0.2
g <- 100
lambdas <- rbeta(1000, a, b)
counts <- sapply(g*lambdas, function(l) {rpois(1, lambda = l)})
hist(
    counts,
    col = "grey50",
    xlab = "Read Counts",
    main = "Poisson-Beta"
)

```
$$\mu=g*a/(a+b)$$
$$\delta^2=g^2*a*b/((a+b+1)*(a+b)^2)$$

_a_: the rate of activation of transcription;

_b_: the rate of inhibition of transcription; 

_g_: the rate of transcript production while transcription is active at the locus.

Differential expression methods may test each of the parameters for differences across groups or only one (often _g_). Implemented in BPSC.

May be further expanded to explicitly account for other sources of gene expression differences such as batch-effect or library depth depending on the particular DE algorithm.

----------

## Under the hood

### MAST
- uses _generalized linear hurdle model_
- designed to account for stochastic dropouts and bimodal expression distribution in which expression is either strongly non-zero or non-detectable
- the rate of expression _Z_, and the level of expression _Y_, are modeled for each gene _g_, indicating whether gene _g_ is expressed in cell _i_ (i.e., $Z_{ig}=0$ if $y_{ig}=0$ and $z_{ig}=1$ if $y_{ig}>0$)
- A *logistic regression model* for the discrete variable _Z_ and a _Gaussian linear model_ for the continuous variable (Y|Z=1):
    $logit (P_r(Z_{ig}=1))=X_i\beta_g^D$
    $P_r(Y_{ig}=Y|Z_{ig}=1)=N(X_i\beta_g^C,\sigma_g^2)$, where $X_i$ is a design matrix

- Model parameters are _fitted_ using an empirical Bayesian framework
- Allows for a joint estimate of nuisance and treatment effects
- DE is determined using _the likelihood ratio test_

### SCDE
- _models_ the read counts for each gene using a mixture of a NB, negative binomial, and a Poisson distribution
- _NB distribution_ models the transcripts that are amplified and detected
- _Poisson distribution_ models the unobserved or background-level signal of transcripts that are not amplified (e.g. dropout events)
- subset of robust genes is used to fit, via _EM_ algorithm, the parameters to the mixture of models
For DE, the posterior probability that the gene shows a fold expression difference between two conditions is computed using a _Bayesian approach_

### Monocole
- Originally designed for ordering cells by progress through differentiation stages (pseudo-time)
- The mean expression level of each gene is _modeled with a GAM_, generalized additive model, which relates one or more predictor variables to a response variable as $g(E(Y))=\beta_0+f_1(x_1)+f_2(x_2)+...+f_m(x_m)$ where Y is a specific gene expression level, $x_i$ are predictor variables, _g_ is a link function, typically log function, and $f_i$ are non-parametric functions (e.g. cubic splines)
- The observable expression level Y is then modeled using GAM,
$E(Y)=s(\varphi_t(b_x, s_i))+\epsilon$ where $\varphi_t(b_x, s_i)$ is the assigned pseudo-time of a cell and $s$ is a cubic smoothing function with three degrees of freedom. The error term $\epsilon$ is normally distributed with a mean of zero
- The DE test is performed using an _approx. $\chi^2$ likelihood ratio test_

## Live coding session


```{r}
x2 <- rnorm(1000, 173, 1)
xfit2<-seq(min(x2),max(x2),length=40) 
yfit2<-dnorm(xfit2,mean=mean(x2),sd=sd(x2)) 
yfit2 <- yfit2*diff(h$mids[1:2])*length(x2) 

```


---------

## ToDo 
_working notes, not part of the tutorial, will be removed_

- improve the session
- add tutorials / coding sessions



## [Back to main](../README.md)
## [Next to Methods Evaluation](session-de-methods-evaluation.md)







